{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6eLuzZ4tPr1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d3224d4-88bf-476a-b5f5-37c8f705a33d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import cv2\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/data'\n",
        "img_size = (240, 310)\n",
        "batch_size = 20"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_in_color(txt_msg, fore_tupple=(0,255,255), back_tupple=(100,100,100)):\n",
        "    rf, gf, bf = fore_tupple\n",
        "    rb, gb, bb = back_tupple\n",
        "    msg = '{0}' + txt_msg\n",
        "    mat = '\\33[38;2;' + str(rf) + ';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' + str(gb) + ';' + str(bb) + 'm'\n",
        "    print(msg.format(mat), flush=True)\n",
        "    print('\\33[0m', flush=True)"
      ],
      "metadata": {
        "id": "YxBHxGEE2Pwb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataframes(sdir):\n",
        "    filepaths = []\n",
        "    labels = []\n",
        "    classlist = sorted(os.listdir(sdir))\n",
        "    for klass in classlist:\n",
        "        classpath = os.path.join(sdir, klass)\n",
        "        if os.path.isdir(classpath):\n",
        "            flist = sorted(os.listdir(classpath))\n",
        "            desc = f'{klass:25s}'\n",
        "            for f in tqdm(flist, ncols=130, desc=desc, unit='files', colour='blue'):\n",
        "                fpath = os.path.join(classpath, f)\n",
        "                filepaths.append(fpath)\n",
        "                labels.append(klass)\n",
        "    Fseries = pd.Series(filepaths, name='filepaths')\n",
        "    Lseries = pd.Series(labels, name='labels')\n",
        "    df = pd.concat([Fseries, Lseries], axis=1)\n",
        "    train_df, dummy_df = train_test_split(df, train_size=.7, shuffle=True, random_state=123, stratify=df['labels'])\n",
        "    valid_df, test_df = train_test_split(dummy_df, train_size=.5, shuffle=True, random_state=123, stratify=dummy_df['labels'])\n",
        "    print('train_df length: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))\n",
        "    return train_df, test_df, valid_df"
      ],
      "metadata": {
        "id": "ljL3QF4c2MJ9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df, valid_df = make_dataframes(base_dir)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "valid_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_dataframe(\n",
        "    train_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
        "    class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size\n",
        ")\n",
        "\n",
        "valid_gen = valid_test_datagen.flow_from_dataframe(\n",
        "    valid_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
        "    class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_gen = valid_test_datagen.flow_from_dataframe(\n",
        "    test_df, x_col='filepaths', y_col='labels', target_size=img_size,\n",
        "    class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGS70CI92KvX",
        "outputId": "ec9a2bc0-4ab3-425a-84b6-0eae8812b4dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "drive                    : 100%|\u001b[34m███████████████████████████████████████████████████████\u001b[0m| 1224/1224 [00:00<00:00, 687351.47files/s]\u001b[0m\n",
            "legglance-flick          : 100%|\u001b[34m███████████████████████████████████████████████████████\u001b[0m| 1120/1120 [00:00<00:00, 531104.63files/s]\u001b[0m\n",
            "pullshot                 : 100%|\u001b[34m███████████████████████████████████████████████████████\u001b[0m| 1260/1260 [00:00<00:00, 662590.65files/s]\u001b[0m\n",
            "sweep                    : 100%|\u001b[34m███████████████████████████████████████████████████████\u001b[0m| 1174/1174 [00:00<00:00, 646803.22files/s]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_df length:  3344   test_df length:  717   valid_df length:  717\n",
            "Found 3343 validated image filenames belonging to 4 classes.\n",
            "Found 717 validated image filenames belonging to 4 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py:1137: UserWarning: Found 1 invalid image filename(s) in x_col=\"filepaths\". These filename(s) will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 717 validated image filenames belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(img_size, num_classes, lr=0.001):\n",
        "    base_model = EfficientNetB3(include_top=False, weights=\"imagenet\", input_shape=(*img_size, 3), pooling='max')\n",
        "    base_model.trainable = True\n",
        "    x = base_model.output\n",
        "    x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\n",
        "    x = Dense(256, kernel_regularizer=keras.regularizers.l2(0.016),\n",
        "              activity_regularizer=keras.regularizers.l1(0.006),\n",
        "              bias_regularizer=keras.regularizers.l1(0.006), activation='relu')(x)\n",
        "    x = Dropout(rate=.4, seed=123)(x)\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "G-g4G5cG2GS-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LR_ASK(keras.callbacks.Callback):\n",
        "    def __init__(self, model, epochs, ask_epoch, dwell=True, factor=.4):\n",
        "        super(LR_ASK, self).__init__()\n",
        "        self.model = model\n",
        "        self.ask_epoch = ask_epoch\n",
        "        self.epochs = epochs\n",
        "        self.ask = True\n",
        "        self.dwell = dwell\n",
        "        self.factor = factor\n",
        "        self.lowest_vloss = np.inf\n",
        "        self.best_weights = self.model.get_weights()\n",
        "        self.best_epoch = 1\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        vloss = logs.get('val_loss')\n",
        "        if vloss < self.lowest_vloss:\n",
        "            self.lowest_vloss = vloss\n",
        "            self.best_weights = self.model.get_weights()\n",
        "            self.best_epoch = epoch + 1\n",
        "            print_in_color(f'\\nValidation loss of {vloss:.4f} is below lowest loss, saving weights from epoch {self.best_epoch}', (0,255,0))\n",
        "        else:\n",
        "            print_in_color(f'\\nValidation loss of {vloss:.4f} is above lowest loss of {self.lowest_vloss:.4f}, keeping weights from epoch {self.best_epoch}', (255,255,0))\n",
        "            if self.dwell:\n",
        "                lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
        "                new_lr = lr * self.factor\n",
        "                print_in_color(f'Learning rate adjusted from {lr:.6f} to {new_lr:.6f}, model weights set to best weights', (0,255,255))\n",
        "                tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n",
        "                self.model.set_weights(self.best_weights)\n",
        "\n",
        "        if self.ask and epoch + 1 == self.ask_epoch:\n",
        "            print_in_color('\\nEnter H to end training or an integer for the number of additional epochs to run then ask again')\n",
        "            ans = input()\n",
        "            if ans.lower() == 'h':\n",
        "                self.model.stop_training = True\n",
        "            else:\n",
        "                self.ask_epoch += int(ans)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        self.model.set_weights(self.best_weights)"
      ],
      "metadata": {
        "id": "2T2uxsed2EEG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(train_gen.class_indices)\n",
        "model = make_model(img_size, num_classes)\n",
        "\n",
        "epochs = 20\n",
        "ask_epoch = 30 #so the callback doesnt trigger\n",
        "ask = LR_ASK(model, epochs, ask_epoch)\n",
        "callbacks = [ask]\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen, epochs=epochs, verbose=1, callbacks=callbacks, validation_data=valid_gen,\n",
        "    validation_steps=None, shuffle=False, initial_epoch=0\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsVp8brB2B3o",
        "outputId": "725c1bb3-993e-45fd-9e3a-49601eb007ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "158/168 [===========================>..] - ETA: 30s - loss: 5.9483 - accuracy: 0.7997"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tr_plot(tr_data, start_epoch):\n",
        "    tacc = tr_data.history['accuracy']\n",
        "    tloss = tr_data.history['loss']\n",
        "    vacc = tr_data.history['val_accuracy']\n",
        "    vloss = tr_data.history['val_loss']\n",
        "    Epoch_count = len(tacc) + start_epoch\n",
        "    Epochs = [i for i in range(start_epoch, Epoch_count)]\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,8))\n",
        "    ax1.plot(Epochs, tloss, 'r', label='Training loss')\n",
        "    ax1.plot(Epochs, vloss, 'g', label='Validation loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax2.plot(Epochs, tacc, 'r', label='Training Accuracy')\n",
        "    ax2.plot(Epochs, vacc, 'g', label='Validation Accuracy')\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "tr_plot(history, 0)"
      ],
      "metadata": {
        "id": "WbrtSW_42ANa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictor(test_gen):\n",
        "    y_pred = []\n",
        "    y_true = test_gen.labels\n",
        "    classes = list(test_gen.class_indices.keys())\n",
        "    class_count = len(classes)\n",
        "    errors = 0\n",
        "    preds = model.predict(test_gen, verbose=1)\n",
        "    tests = len(preds)\n",
        "    for i, p in enumerate(preds):\n",
        "        pred_index = np.argmax(p)\n",
        "        true_index = test_gen.labels[i]\n",
        "        if pred_index != true_index:\n",
        "            errors += 1\n",
        "        y_pred.append(pred_index)\n",
        "\n",
        "    acc = (1 - errors / tests) * 100\n",
        "    print_in_color(f'There were {errors} errors in {tests} tests for an accuracy of {acc:.2f}%', (0,255,255))\n",
        "\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_true = np.array(y_true)\n",
        "    f1score = f1_score(y_true, y_pred, average='weighted') * 100\n",
        "\n",
        "    if class_count <= 30:\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)\n",
        "        plt.xticks(np.arange(class_count)+.5, classes, rotation=90)\n",
        "        plt.yticks(np.arange(class_count)+.5, classes, rotation=0)\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"Actual\")\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.show()\n",
        "\n",
        "    clr = classification_report(y_true, y_pred, target_names=classes, digits=4)\n",
        "    print(\"Classification Report:\\n----------------------\\n\", clr)\n",
        "    return errors, tests, f1score"
      ],
      "metadata": {
        "id": "4CoB-Imy1-dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors, tests, f1score = predictor(test_gen)\n",
        "name = f'CRICKET-{len(train_gen.class_indices)}-({img_size[0]}x{img_size[1]})'\n",
        "save_id = f'{name}-{f1score:.2f}.h5'\n",
        "model_save_loc = os.path.join('/content/drive/MyDrive', save_id)\n",
        "model.save(model_save_loc)\n",
        "print_in_color(f'Model was saved as {model_save_loc}', (0,255,255))"
      ],
      "metadata": {
        "id": "4Jnda_76118o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gSaJtIS20Coj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}