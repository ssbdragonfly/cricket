{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61JnOTXwTtxF",
        "outputId": "5bf04c9a-dc92-4223-e749-5415c41430c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/cricshot'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames(video_path, num_frames=16):\n",
        "    cap = tf.io.read_file(video_path)\n",
        "    cap = tf.io.decode_video(cap)\n",
        "    total_frames = tf.shape(cap)[0]\n",
        "    if total_frames < num_frames:\n",
        "        indices = tf.range(total_frames)\n",
        "        indices = tf.repeat(indices, tf.cast(tf.ceil(num_frames / total_frames), tf.int32))\n",
        "        indices = indices[:num_frames]\n",
        "    else:\n",
        "        indices = tf.linspace(0, total_frames - 1, num_frames)\n",
        "        indices = tf.cast(indices, tf.int32)\n",
        "\n",
        "    frames = tf.gather(cap, indices)\n",
        "    frames = tf.image.resize(frames, (224, 224))\n",
        "    frames = frames / 255.0\n",
        "    return frames"
      ],
      "metadata": {
        "id": "CA0i6ja3T8ts"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataframes(base_dir, num_frames=16):\n",
        "    filepaths = []\n",
        "    labels = []\n",
        "    classlist = sorted(os.listdir(base_dir))\n",
        "    for klass in classlist:\n",
        "        classpath = os.path.join(base_dir, klass)\n",
        "        if os.path.isdir(classpath):\n",
        "            flist = sorted([f for f in os.listdir(classpath) if f.endswith('.mp4')])\n",
        "            desc = f'{klass:25s}'\n",
        "            for f in tqdm(flist, ncols=130, desc=desc, unit='files', colour='blue'):\n",
        "                fpath = os.path.join(classpath, f)\n",
        "                filepaths.append(fpath)\n",
        "                labels.append(klass)\n",
        "\n",
        "    df = pd.DataFrame({'filepaths': filepaths, 'labels': labels})\n",
        "\n",
        "    train_df, temp_df = train_test_split(df, train_size=0.7, stratify=df['labels'], random_state=42)\n",
        "    valid_df, test_df = train_test_split(temp_df, train_size=0.5, stratify=temp_df['labels'], random_state=42)\n",
        "\n",
        "    print('train_df length:', len(train_df), '  valid_df length:', len(valid_df), '  test_df length:', len(test_df))\n",
        "    return train_df, valid_df, test_df\n",
        "\n",
        "train_df, valid_df, test_df = make_dataframes(base_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "vP7K2mivURFj",
        "outputId": "f0dac854-30f4-42a3-de9c-0892837989e6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "cover                    : 0files [00:00, ?files/s]\n",
            "defense                  : 0files [00:00, ?files/s]\n",
            "flick                    : 0files [00:00, ?files/s]\n",
            "hook                     : 0files [00:00, ?files/s]\n",
            "late_cut                 : 0files [00:00, ?files/s]\n",
            "lofted                   : 0files [00:00, ?files/s]\n",
            "pull                     : 0files [00:00, ?files/s]\n",
            "square_cut               : 0files [00:00, ?files/s]\n",
            "straight                 : 0files [00:00, ?files/s]\n",
            "sweep                    : 0files [00:00, ?files/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=None and train_size=0.7, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-9f1bdf576d87>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Create dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-9f1bdf576d87>\u001b[0m in \u001b[0;36mmake_dataframes\u001b[0;34m(base_dir, num_frames)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Split the data into train, validation, and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=None and train_size=0.7, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoDataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, dataframe, batch_size, num_frames, num_classes, shuffle=True):\n",
        "        self.df = dataframe\n",
        "        self.batch_size = batch_size\n",
        "        self.num_frames = num_frames\n",
        "        self.num_classes = num_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.df) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_df = self.df.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_videos = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for _, row in batch_df.iterrows():\n",
        "            video = extract_frames(row['filepaths'], self.num_frames)\n",
        "            label = keras.utils.to_categorical(row['label_index'], num_classes=self.num_classes)\n",
        "\n",
        "            batch_videos.append(video)\n",
        "            batch_labels.append(label)\n",
        "\n",
        "        return np.array(batch_videos), np.array(batch_labels)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "QJalmk93UVPK",
        "outputId": "175b7579-dded-48e1-da0c-052e62f54654"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/drive/folders/1DPHURwQk5R8blgjM8VNz6Q68LqckxljX'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8297fc6ab817>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-37fb39a178d8>\u001b[0m in \u001b[0;36mmake_dataframes\u001b[0;34m(base_dir, num_frames)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfilepaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclasslist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasslist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mclasspath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/drive/folders/1DPHURwQk5R8blgjM8VNz6Q68LqckxljX'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = sorted(train_df['labels'].unique())\n",
        "class_to_index = {cls: idx for idx, cls in enumerate(classes)}\n",
        "\n",
        "for df in [train_df, valid_df, test_df]:\n",
        "    df['label_index'] = df['labels'].map(class_to_index)\n",
        "num_classes = len(classes)\n",
        "batch_size = 16\n",
        "num_frames = 16\n",
        "\n",
        "train_gen = VideoDataGenerator(train_df, batch_size, num_frames, num_classes)\n",
        "valid_gen = VideoDataGenerator(valid_df, batch_size, num_frames, num_classes, shuffle=False)\n",
        "test_gen = VideoDataGenerator(test_df, batch_size, num_frames, num_classes, shuffle=False)"
      ],
      "metadata": {
        "id": "d1N7zLsDUZL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(num_frames, img_size, num_classes, lr=0.001):\n",
        "    input_shape = (num_frames, *img_size, 3)\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=input_shape),\n",
        "        keras.layers.MaxPooling3D((2, 2, 2)),\n",
        "        keras.layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
        "        keras.layers.MaxPooling3D((2, 2, 2)),\n",
        "        keras.layers.Conv3D(128, (3, 3, 3), activation='relu'),\n",
        "        keras.layers.MaxPooling3D((2, 2, 2)),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(256, activation='relu'),\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = make_model(num_frames, (224, 224), num_classes)"
      ],
      "metadata": {
        "id": "nCJQsRnMV_aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "epochs = 20\n",
        "callbacks = [early_stop, reduce_lr]\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=valid_gen,\n",
        "    validation_steps=None,\n",
        "    shuffle=False,\n",
        "    initial_epoch=0\n",
        ")"
      ],
      "metadata": {
        "id": "x1b6u9h0WByD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tr_plot(tr_data, start_epoch):\n",
        "    tacc = tr_data.history['accuracy']\n",
        "    tloss = tr_data.history['loss']\n",
        "    vacc = tr_data.history['val_accuracy']\n",
        "    vloss = tr_data.history['val_loss']\n",
        "    Epoch_count = len(tacc) + start_epoch\n",
        "    Epochs = [i for i in range(start_epoch, Epoch_count)]\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "    ax1.plot(Epochs, tloss, 'r', label='Training loss')\n",
        "    ax1.plot(Epochs, vloss, 'g', label='Validation loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax2.plot(Epochs, tacc, 'r', label='Training Accuracy')\n",
        "    ax2.plot(Epochs, vacc, 'g', label='Validation Accuracy')\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "tr_plot(history, 0)"
      ],
      "metadata": {
        "id": "lJOZI2cgWEPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictor(test_gen):\n",
        "    y_pred = []\n",
        "    y_true = test_gen.df['label_index'].values\n",
        "    classes = list(test_gen.df['labels'].unique())\n",
        "    class_count = len(classes)\n",
        "    errors = 0\n",
        "    preds = model.predict(test_gen, verbose=1)\n",
        "    tests = len(preds)\n",
        "    for i, p in enumerate(preds):\n",
        "        pred_index = np.argmax(p)\n",
        "        true_index = test_gen.df['label_index'].iloc[i]\n",
        "        if pred_index != true_index:\n",
        "            errors += 1\n",
        "        y_pred.append(pred_index)\n",
        "\n",
        "    acc = (1 - errors / tests) * 100\n",
        "    print(f'There were {errors} errors in {tests} tests for an accuracy of {acc:.2f}%')\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_true = np.array(y_true)\n",
        "    f1score = f1_score(y_true, y_pred, average='weighted') * 100\n",
        "    if class_count <= 30:\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)\n",
        "        plt.xticks(np.arange(class_count) + .5, classes, rotation=90)\n",
        "        plt.yticks(np.arange(class_count) + .5, classes, rotation=0)\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"Actual\")\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.show()\n",
        "\n",
        "    clr = classification_report(y_true, y_pred, target_names=classes, digits=4)\n",
        "    print(\"Classification Report:\\n----------------------\\n\", clr)\n",
        "    return errors, tests, f1score\n",
        "\n",
        "errors, tests, f1score = predictor(test_gen)"
      ],
      "metadata": {
        "id": "0a1ftYoKWKCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = f'CRICKET-{len(train_gen.df[\"labels\"].unique())}-({224}x{224})'\n",
        "save_id = f'{name}-{f1score:.2f}.h5'\n",
        "model_save_loc = os.path.join('/content/drive/MyDrive', save_id)\n",
        "model.save(model_save_loc)\n",
        "print(f'Model was saved as {model_save_loc}')"
      ],
      "metadata": {
        "id": "PTxZYgGBWNHP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}